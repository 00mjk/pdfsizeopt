% by pts@fazekas.hu at Sat Jul 11 12:01:05 CEST 2009
%
% Overall steps: !!
%
% * efficient $\to$ effective
% * spell check
% * review
% * make it accepted by Googles

\documentclass{ltugproc}
%\usepackage[latin2]{inputenc}% !! check
\usepackage{t1enc}
\usepackage{lmodern}
\usepackage{mflogo}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage[english]{babel}

\def\cmd{\textsf}
\def\pkg{\textsf}

%** Like \caption, but above the table, with \abovecaptionskip and
%** \belowcaptionskip swapped.
\def\captiontop#1{%
  \advance\abovecaptionskip-\belowcaptionskip
  \advance\belowcaptionskip\abovecaptionskip
  \advance\abovecaptionskip-\belowcaptionskip
  \abovecaptionskip-\abovecaptionskip
  \caption{#1}%
  \advance\abovecaptionskip-\belowcaptionskip
  \advance\belowcaptionskip\abovecaptionskip
  \advance\abovecaptionskip-\belowcaptionskip
  \abovecaptionskip-\abovecaptionskip
}


% Make \textunderscore and \_ shorter for font family lmr and lmss.
\makeatletter
\edef\pts@getfontfamily#1/#2/#3\hfuzz#4{\def#4{#2}}
\def\pts@@fontfamily@lmr{lmr}
  \edef\pts@@fontfamily@lmr{%
    \expandafter\strip@prefix\meaning\pts@@fontfamily@lmr}
\def\pts@@fontfamily@lmss{lmss}
  \edef\pts@@fontfamily@lmss{%
    \expandafter\strip@prefix\meaning\pts@@fontfamily@lmss}
%** Define #1 to the name of the active font family.
\def\pts@def@fontfamily#1{%
  \expandafter\expandafter\expandafter\pts@getfontfamily
      \expandafter\string\the\font///\hfuzz#1%
}
\let\pts@@orig@textunderscore\textunderscore
\DeclareRobustCommand\textunderscore{%
  \begingroup\pts@def@fontfamily\reserved@a
  \ifx\reserved@a\pts@@fontfamily@lmr
    \endgroup\vrule width1ex height0pt depth0.4pt\relax
  \else\ifx\reserved@a\pts@@fontfamily@lmss
    \endgroup\vrule width1ex height0pt depth0.4pt\relax
  \else
    \endgroup\pts@@orig@textunderscore
  \fi\fi
}

\title{Optimizing PDF output size of \TeX{} documents}
\author{P\'eter Szab\'o}
\address{Google\\
  Brandschenkestrasse 110\\
  CH-8002, Z\"urich, Switzerland}
\netaddress{pts@google.com}
\personalURL{http://www.inf.bme.hu/~pts/}

\begin{abstract}
There are several tools to generate PDF output from a \TeX{} document. By
choosing the appropriate tools and configuring them properly, it is possible
to reduce the PDF output size by a factor of 3 or even more, thus reducing
document download times, hosting and archiving costs.
We enumerate the most common tools, and show how to configure them to reduce
the size of text, fonts, images and cross-reference information embedded into
the final PDF. We also analyze image compression in detail.

We present a new tool called \cmd{pdfsizeopt.py} which optimizes the size of
embedded images and Type\,1 fonts, and removes object duplicates. We also
propose a workflow for PDF size optimization, which involves configuration
of \TeX{} tools, running \cmd{pdfsizeopt.py} and the Multivalent PDF
compressor as well.

\end{abstract}

\begin{document}

\maketitle

\section{Introduction}

!! Should I include ``This work was funded by my employer, Google.''??

\subsection{What does a PDF document contain}

PDF is a popular document file format designed for printing and on-screen
viewing. PDF faithfully preserves the design elements of the document, such
as fonts, line breaks, page breaks, exact spacing, text layout, vector
graphics and image resolution. Thus the author of a PDF document has precise
control over the document's appearance -- no matter what operating system or
renderer software is used for viewing or printing the PDF. From the viewer's
prespective, a PDF document is a sequence of rectangular pages containing
text, vector graphics and pixel-based images. In addition to that, some
rectangular page regions can be marked as hyperlinks, and Unicode
annotations can be added as well to the regions, so text can be copy-pasted
from the documents. (Usually the copy-paste yields only a sequence of
characters, with all formatting and positioning lost. Depending on the
software and the annotation, the bold and italics properties can be
preserved.) A table of contents can be added as well, which has a tree
structure, and each node of the tree consists of an unformatted caption and
a hyperlink within the document.

Additional features of PDF include forms (the user fills some fields with
data, clicks on the submit button, and the data is sent to a server in an
HTTP request), event handlers in JavaScript, embedded multimedia files,
encryption and access protection.

\subsection{How to create PDF}

Since PDF doesn't contain structural and semantic information (such as
in which order the document should be read, which regions are titles, how
are the tables built and how are the charts generated), word processors and
typesetting systems usually can export to PDF, but they use their own file
format which preserves semantics. PDF is usually not involved while the
author is composing (or typeseting) the document, but once a version of a
document is ready, a PDF can be exported and distributed. Should the author
distribute the document in the native file format of the word processor, he
might risk that the document doesn't get rendered as he intended, due to
software version differences or because slightly different fonts are
installed on the rendering computer, or the page layout settings in the word
processor are different.

Most word processors and drawing programs and image editors support
exporting as PDF. It is also possible to generate a PDF even if the software
doesn't have a PDF export feature. For example, it may be possible to
install a printer driver, which generates PDF instead of sending the
document to a real printer. (For example, on Windows, PDFCreator
\cite{pdfcreator} is such an open-source driver.) Some old programs can emit
PostScript, but not PDF. The \cmd{ps2pdf} \cite{ps2df} tool (part of
Ghostcript) can be used to convert the PostScript to PDF.

There are several options for PDF generation from
\TeX{} documents, including
pdf\TeX{}, \cmd{dvipdfmx} and \cmd{dvips} $+$ \cmd{ps2pdf}. Depending on how
the document uses hyperlinks and PostScript programming in graphics, some of
these would not work. See the details in Subsection~\ref{tex-to-pdf}.

\subsection{Motivation for making PDF files smaller}

Our goal is to reduce the size of PDF files, focusing on those
created from \TeX{} documents. Having smaller PDF files reduces download
times, web hosting costs and storage costs as well. Although there is no
urgent need for reducing PDF storage costs for personal use (since hard
drives in modern PCs are large enough), storage costs are significant for
publishing houses, print shops, e-book stores and hosting services,
libraries and archives \cite{multivalent-article}.
Usually lots of copies and backups are made of PDF
files originating from such places, saving 20\% of the file size right after
generating the PDF would save 20\% of all future costs associated with the
file.

Although e-book readers can store lots of documents (e.g.\ a 4\,GB e-book
reader can store 800 PDF books of 5\,MB average reasonable file size), they
get full quickly if we don't pay attention to optimized PDF generation. One
can easily get a PDF file 5 times larger than reasonable by genertating it
with a software which doesn't pay attention to size,
or not setting the export settings
properly. Upgrading or changing the generator software is not always
feasible. A PDF recompressor becomes useful in these cases.

It is not our goal to propose or use
alternative file formats, which support a more
compact document representation or more agressive compression than PDF. An
example for such an approach is the Multivalent \emph{compact} file format
\cite{multivalent-compact}, which can be generated from a PDF using the
Multivalent tools, and it can be converted back to a regular PDF with these
tools as well. One of the merits of PDF is that it has a definitive, widely
accepted and implemented, freely available
specification \cite{pdfref} (and version 1.7 is
even ISO standard \cite{pdf-iso}), so if 20 years later we want to
print the document we've create today, then a well-documented and
standardized format such as PDF is a natural choice. (Please note, however,
that the PDF specification is not self-contained, it refers to other
specifications, e.g.\ for some compression algorithms and font formats.)
Another alternative document format is is DjVu (see in
Section~\ref{related-work}).

It is possible to save space in a PDF by removing non-printed information
such as hyperlinks, document outline elements, forms, text-to-Unicode
mapping or user annotations. Removing these does not affect the output when
the PDF is printed, but it degrades the user experience when the PDF is
viewed on a computer, and it may also degrade navigation and searchability.
Another option is to remove (unembed fonts). In such a case, the PDF viewer
will pick a font with similar metrics if the font is not installed on the
viewer machine. Please note that unembedding the font doesn't change the
horizontal distance between glyphs, so the page layout will remain the
same, but maybe glpyhs will look funny or hard-to-read. Yet another option
to save space is to reduce the resolution of the embedded images. We will
not use any of the techniques mentioned in this paragraph, because our goal
is to reduce redundancy and make the byte representation more effective,
while preserving visual and semantic information in the document.

\subsection{PDF file structure}

It is possible to save space in the PDF by serializing the same information
more effectively and/or using better compression. This section gives a
high-level introductions to the data structures and their serialization in
the PDF file, focusing on size optimization. For a full description of the
PDF file format, see \cite{pdfref}.

PDF supports integer, real number, boolean, null, string and name as
simple data types. A string a sequence of 8-bit bytes. A name is also a
sequence of 8-bit bytes, usually a concatenation of a few English words in
CamelCase, often used as a dictionary key (e.g. \texttt{/MediaBox}) or an
enumeration value (e.g. \texttt{/DeviceGray}). Composite data types are the
list and the dictionary. A dictionary is an unordered sequence of key--value
pairs, where keys must be names. Values in dictionaries and list items can
be primitive or composite. There is a simple serialization of values to
8-bit strings, compatible with PostScript LanguageLevel\,2. For example,
\texttt{\hbox{<}</Integer 5 /Real -6.7 /String ((C)2009\textbackslash))
/StringInHex <Face> /Null null
/Boolean true /Name /Foo /List [3 4 5]\hbox{>}>} defines a dictionary
containing values of various types. All data types are immutable.

It is possible to define a value for future use by defining an
\emph{object.} For example, \texttt{12 0 obj [/PDF/Text] endobj} defines
object number 12 to be an array of two items (\texttt{/PDF} and
\texttt{/Text}). The number 0 in the definition is the so-called generation
number, referring to version of the incrementally updated PDF file this
object was created in. Since most of the tools just create a new PDF instead
of updating parts of an existing one, we can assumme for simplicity that the
generation number is always zero. Once an object is defined it is possible
to refer to it (e.g.\ \texttt{12 0 R}) instead of typing its value. It is
possible to define self-referential lists and dictionaries using object
definitions. The PDF specification requires some PDF structure elements
(such as the \texttt{/FontDescriptor} value) be an indindirect reference,
i.e.\ defined as an object. Such elements cannot be inlined into other
object, but they must be referred to.

A PDF file contains a header, a list of objects, a \emph{trailer}
dictionary, cross-reference information (offsets of object definitions,
sorted by object number), and the end-of-file marker. The header contains
the PDF version (PDF-1.7 being the latest). All of the file elements above
except for the PDF version, the list of objects and the trailer are
redundant, and can be regegenerated if lost. The parsing of the PDF starts
at the trailer dictionary. Its \texttt{/Root} value refers to the catalog
dictionary object, whose \texttt{/Pages} value refers to a dictionary object
containing the list of pages. The interpretation of each object depends on
the reference path which leads to that object from the trailer. In addition
to that, dicitionary objects may have the \texttt{/Type} and/or
\texttt{/Subtype} value indicating the interpretation. For example,
\texttt{\hbox{<}</Subtype/Image ...\hbox{>}>} defines a pixel-based image.

In addition to the data types above, PDF supports streams as well. A
\emph{stream} object is a dictionary augmented by the stream data, which is
a byte sequence. The syntax is \texttt{... ... obj << ... >> stream ...
endstream endobj}, where the stream data starts right after the keyword
\texttt{stream}. The stream data can be compressed or otherwise encoded
(such as in hex). The \texttt{/Filter} and \texttt{/DecodeParms} values in
the dictionary specify how the uncompress/\allowbreak decode the stream data.
It possible the specify multiple uncompress/\allowbreak decode filters,
e.g.\ \texttt{/Filter [/ASCIIHexDecode /FlateDecode]} says that the bytes
aftter \texttt{stream} should be decoded as a hex string, and then
uncompressed using PDF's ZIP implementation. (Please note that the use of
\texttt{/ASCIIHexDecode} is just a waste of space unless one wants to create
an ASCII PDF file.) The three most common uses for
streams are: image pixel data, embedded font files and content streams.
A content stream contains the instructions to draw the contents of page. The
stream data is ASCII, with a syntax similar to PostScript, but with
different operators. For example, \texttt{BT/F 20 Tf 1 0 0 1 8 9
Tm(Hello world)Tj ET} in a content stream draws the text ``Hello World''
with the font \texttt{/F} at size 20 units, shifted up by 8 units, and
shifted right by 9 units (according to the transformation matrix
\texttt{1 0 0 1 8 9}).

Streams can use the following generic compression methods: ZIP (also called
as flate), LZW and run-length encoding. ZIP is almost always superior. In
addition to those, PDF supports some image-specific compression methods as
well: JPEG and JPEG2000 for true-color images and JBIG2 and G3 fax (also
called as CCITT fax) for bilevel (two-color) images. JPEG and JPEG2000 are
lossy methods, they usually yield the same size at the same quality
settings -- but JPEG2000 is more flexible. JBIG2 is superior to G3 fax and
ZIP for bilevel images. Any number of compression filters can be applied to
a stream, but usually applying more than one yields a larger compressed
stream size than just applying one. ZIP and LZW support predicors as well. A
predictor is an easy-to-compute, invertible filter which is applied to the
stream data before compression, to make the data more compressible. One
possible predictor subtracts the previous data value from the current one,
and sends the difference to the compressor. This helps reducing the file
size if the difference between adjacent data values is small most of the
time. This is true for some images with a few number of colors.

There is cross-reference information near the end of the PDF file, which
contains the start byte offset of all object definitions. Using this
information it is possible to render parts of the file, without reading the
whole file. The most common format for cross-reference information is the
\emph{cross-reference table} (starting with the keyword \texttt{xref}). Each item
in the table consumes 20 bytes, and contains an object byte offset. The
object number is encoded by the position of the item. For PDFs with several
thousand objects, the space occupied by the cross-reference table is is not
negligable. PDF\,1.5 introduces \emph{cross-reference streams,} which store
the cross-reference information in compact form in a stream. Such streams
usually compressed as well, using ZIP and a predictor. The benefit of the
predictor is that adjacent offsets are close to each other, so their
difference will contain lots of zeroes, which can be compressed better.

Compression cannot be applied to the PDF file as a whole, only individual
parts (such as stream data and cross-reference information) can be
compressed. However, there can be lots of small object definitions in the
file which are not streams. To compress those, PDF\,1.5 introduces
\emph{object streams.} The data in an object stream contains a concatenation
of any number of non-stream object definitions. Object streams can be
compressed just as regular stream data. This makes it possible to squeeze
repetitions spanning over multiple object definitions. Thus, with PDF\,1.5,
most of the PDF file can be stored in compressed streams. Only a few dozen
byte of headers and end-of-file markers, and the stream dictionaries remain
uncompressed.

\section{Making PDF files smaller}

\subsection{How to prepare a small, optimizable PDF with \TeX{}}\label{tex-to-pdf}

When aiming for a small PDF, it is possible the get it by using the best
tools with the proper settings to create the smallest possible PDF right
ahead. Another approach is to create a PDF anyhow, not paying attention to
the tools and their settings, and then optimize PDF with a PDF size
optimizer tool. The approach we suggest in this work a mixture of to two:
pay attention to the PDF generator tools and its fundamental settings, so it
generates a PDF which is small enough for temporary use and also
easy to optimize further; and use an optimizer to create the final, smallest
possible PDF.

This section enumerates the most common tools which can generate the
temporary PDF from a \texttt{.tex} source. As part of this, it explains how
to enforce the proper compression and font settings, and how to
prepare vector and pixel-based images so they don't become unnecessarily
large.

\paragraph{Pick the best PDF generation method}
Table~\ref{tab:method-feature} lists features of the 3 most common methods
(also called as \emph{drivers}) which produce a PDF from a \TeX{} document,
and Table~\ref{tab:texbook-to-pdf} compares the file size whey produce when
compiling the \TeX{}book. There is no single best driver because of the
different feature sets, but looking at how large the output of \cmd{dvips}
is, the preliminary conclusion would be to use pdf\TeX{}
or \cmd{dvipdfm(x)} except if advanced PostScript features are needed (such
as for \pkg{psfrag} and \pkg{pstricks}).

\begin{table}
\captiontop{Output file sizes of PDF generation from The \TeX{}book, with
various methods. The PDF was optimized with \cmd{pdfsizeopt.py}, then with
Multivalent}\label{tab:texbook-to-pdf}
\par\small\noindent\hfil
\begin{tabular}{@{}lrr@{}}
\toprule
                  &&\emph{optimized}\\
\emph{method}     &\emph{PDF bytes}&\emph{PDF bytes}\\\midrule
pdf\TeX           &2283510 &1806887\\
\cmd{dvipdfm}     &2269821 &1787039\\
\cmd{dvipdfmx}    &2007012 &1800270\\
\cmd{dvips}$+$\cmd{ps2pdf}      &3485081 &3181869\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}
\captiontop{Features supported by various PDF output
methods}\label{tab:method-feature}
\par\small\noindent\hfil
\begin{tabular}{@{}llll@{}}
\toprule
\emph{Feature}        & pdf\TeX   & \cmd{dvipdfm(x)} & \cmd{dvips} \\\midrule
\pkg{hyperref}        & +         & +                & +           \\
TikZ                  & +         & +                & +           \\
\pkg{beamer.cls}      & +         & $+^o$            & $+^u$       \\
include PDF           & +         & $+^b$            & +           \\
embed bitmap font     & +         & +                & +           \\
embed Type\,1 font    & +         & +                & +           \\
[2ex]
embed TrueType font   & +         & +                & $-$         \\
include EPS           & $-$       & +                & +           \\
include JPEG          & +         & $+^x$            & $-$         \\
include PNG           & +         & $+^x$            & $-$         \\
include \MP           & $+^m$     & $+^m$            & $+^r$       \\
\pkg{psfrag}          & $-^f$     & $-^f$            & +           \\
\pkg{pstricks}        & $-^f$     & $-^f$            & +           \\
\pkg{pdfpages}        & +         & $-$              & $-$         \\
line break in link    & +         & +                & $-$         \\
\bottomrule
\end{tabular}
\par\bigskip
\par\noindent \emph{b:} bounding box detection with \cmd{ebb} or
  \pkg{pts-graphics-helper}
\par\noindent \emph{f:} see \cite{pstricks-pdfoutput} for workarounds
\par\noindent \emph{m:} convenient with \texttt{\string\includegraphicsmps}
  defined in \pkg{pts-graphics-helper}
\par\noindent \emph{r:} rename file to \texttt{.eps} manually
\par\noindent \emph{o:} with \texttt{\string\documentclass[dvipdfm]\{beamer\}}
\par\noindent \emph{u:} use \texttt{dvips -t unknown doc.dvi} to get the paper
size right.
\par\noindent \emph{x:} with \texttt{\string\usepackage[dvipdfmx]\{graphics\}}
  and shell escape running \cmd{extractbb}
\end{table}

We continue with presenting and analyzing the methods mentioned.

\begin{description}

\item[dvips] This approach converts
\TeX{} source $\to$ DVI $\to$ PostScript $\to$ PDF,
using \cmd{dvips} \cite{dvips} for creating the PostScript file, and
\cmd{ps2pdf} \cite{ps2pdf} (part of Ghostscript) for creating the PDF file. Example
command-lines for compiling \texttt{doc.tex} to \texttt{doc.pdf}:
\begin{verbatim}
$ latex doc
$ dvips doc
$ ps2pdf14 -dPDFSETTINGS=/prepress doc.ps
\end{verbatim}%$

\item[dvipdfmx] The tool \cmd{dvipdfmx} \cite{dvipdfmx} converts from
DVI to PDF, producing a very small output file. \cmd{dvipdfmx} is part of \TeX{}\,Live 2008, but since it's
quite new, it may be missing from other \TeX{} distributions. Its
precedessor, \cmd{dvipdfm} has not been updated since March 2007. Notable
new features in \cmd{dvipdfmx} are: support for non-latin scripts and fonts;
emitting the Type\,1 fonts in CFF (that's the main reason for the size
difference in Table~\ref{tab:method-feature}); parsing pdf\TeX{}-style font
\texttt{.map} files. Example command-lines:
\begin{verbatim}
$ latex doc
$ dvipdfmx doc
\end{verbatim}

\item[pdf\TeX] The commands \cmd{pdftex} or \cmd{pdflatex}  \cite{pdftex}
generate PDF directly from the \texttt{.tex} source, without any intermediate
files. An important advantage of pdf\TeX{} over the other methods is that it
integrates nicely with the editors \TeX{}\-Shop and \TeX{}works. The
single-step approach ensures that there would be no glitches
(e.g.\ images misaligned or not properly sized)
because the tools are not integrated properly.
Example command-line:
\begin{verbatim}
$ pdflatex doc
\end{verbatim}%$
~

\end{description}

\noindent The command \texttt{latex doc} is run for both \cmd{dvips} and
\cmd{dvipdfm(x)}. Since these two drivers expect a bit different
\texttt{\string\special}s in the DVI file, the driver name has to be
communicated to the \TeX{} macros generating the \texttt{\string\special}s.
For \LaTeX{}, \cmd{dvips} is the default. To get \cmd{dvipdfm(x)} right,
pass \texttt{dvipdfm} (or \texttt{dvipdfmx}) as an option to
\texttt{\string\documentclass} or to both
\texttt{\string\usepackage\{graphicx\}} and
\texttt{\string\usepackage\{hyperref\}}. Loading the package
\pkg{pts-graphics-helper} \cite{pts-graphics-helper} sets up \cmd{dvipdfm}
as default unless the document is compiled with \cmd{pdflatex}.

Unfortunately, some graphics packages (such as \pkg{psfrag} and
\pkg{pstricks}) require a PostScript backend such as \cmd{dvips},
and pdf\TeX{} or \cmd{dvipdfmx} don't provide that. See
\cite{pstricks-pdfoutput} for a list of workarounds. They rely running
\cmd{dvips} on the graphics, possibly converting its output to PDF, and then
including those files in the main compilation. Most of the extra work can be
avoided if graphics are created as external PDF files (without text
replacements), TikZ \cite{tikz} figures or \MP{} figures. TikZ and \MP{}
support text captions typeset by \TeX{}. Inkscape users can use
\pkg{textext} \cite{textext} within Inkscape to make \TeX{} typeset the
captions.

The \texttt{\string\includegraphics} command of the standard \pkg{graphicx}
\LaTeX{}-package accepts a PDF as the image file. In this case, the first
page of the specified PDF will be used as a rectangular image. With
\cmd{dvipdfm(x)}, one also needs a \texttt{.bb} (or \texttt{.bbx}) file
containing the bounding
box. This can be generated with the \cmd{ebb} tool (or \cmd{extractbb},
shipping with
\cmd{dvipdfm(x)}. Or, it is possible to use the \pkg{pts-graphics-helper}
package \cite{pts-graphics-helper}, which can find the PDF bounding box
directly (most of the time).

\pkg{dvipdfm(x)} contains special support for embedding figures created by
\MP{}. For pdf\TeX{}, the \pkg{graphicx} package loads \pkg{supp-pdf.tex},
which can parse the output of \MP{}, and embed it to the document.
Unfortunately, the \pkg{graphicx} package is not smart enough to recognize
\MP{} output files (\texttt{jobname.1}, \texttt{jobname.2} etc.) by
extension. The \pkg{pts-graphics-helper} package \cite{pts-graphics-helper}
overcomes this limitation by defining \texttt{\string\includegraphicsmps},
which can be used in place of \texttt{\string\includegraphics} for including
figures created by \MP{}. The packages works consistently with
\pkg{dvipdfm(x)} and pdf\TeX{}.

With pdf\TeX{}, it is possible to embed page regions from an external PDF,
using the \pkg{pdfpages} \LaTeX{}-package. Please note that due to a
limitation in pdf\TeX{}, hyperlinks and outlines (table of contents) in the
embedded PDF will be lost.

Although \cmd{dvipdfm(x)} supports PNG and JPEG image inclusion, calculating
the bounding box may be cumbersome. It is recommended that all external
images should be converted to PDF first. The recommended software for that
conversion is \cmd{sam2p} \cite{sam2p}, which creates a small PDF (or EPS)
quickly.

Considering all of the above, we recommend using pdf\TeX{} for compiling
\TeX{} documents to PDF. If, for some
reason, using pdf\TeX{} is not feasible, we recommend \cmd{dvipdfmx} from
\TeX{}\,Live 2008 or later. If a 1\% decrease in file size is worth the
trouble of getting fonts right, we we recommend \cmd{dvipdfm}.
In all the cases above, the final PDF should be optimized with
\cmd{pdfsizeopt.py} (see later).

\paragraph{Get rid of complex graphics}

Some computer algebra programs and vector modeling tools emit very large
PDF (or similar vector graphics) files. This can be because they draw the
graphics of too many little parts (e.g.\ they draw a sphere using several
thousand triangles), or they draw too many parts which would
be invisible anyway, because other parts cover them. Converting or
optimizing such PDF files usually doesn't help, because the optimizers are not
smart enough to rearrange the drawing instructions, and then skip some of
them. A good rule of thumb is that if a figure in an optimized PDF file is
larger than the corresponding PNG file rendered in 600\,DPI, then the
figure is too complex. To reduce the file size, it is recommended to 
export the figure as a PNG (or JPEG) image from the program, and embed that
bitmap image.

\paragraph{Downsample high-resolution images}

For most printers it doesn't make a visible difference to print in a
resolution higher than 600\,DPI. Sometimes even the difference between
300\,DPI and 600\,DPI is negligable. So converting the embedded images down
to 300\,DPI may save significant space without too much quality degradation.
Downsampling before the image is included is a bit of manual work
for each image, but there are lot of free software tools to do that (such as
GIMP \cite{gimp} and the \cmd{convert} tool of ImageMagick \cmd{imagemagick}).
It is possible to downsample after the PDF has been created, for example
with the commercial software PDF Enhancher \cite{pdf-enhancer} or Adobe
Acrobat. \cmd{ps2pdf}
(using Ghostscript's \texttt{-dDEVICE=pdfwrite} \cite{pdfwrite-params}) can
read PDF files, and downsample images within as well, but it usually grows
other parts of the file too much (15\% increase in file size for The
\TeX{}book), and it may lose some information (it does
keep hypherlinks and the document outline, though).

\paragraph{Crop large images}

If only parts of a large image contain useful and relevant information, one
can save space by cropping the image.

\paragraph{Choose the JPEG quality}

When using JPEG (or JPEG2000) compression, there is a dradeoff between
quality and file size. Most JPEG encoders based on libjpeg accept an integer
quality value between 1 and 100. For true color photos, a quality below 40
produces a severely degraded, hard-to-recognize image, with 75 we get some
harmless glitches, and with 85 the degradation is hard to notice. If the
document contains lots of large JPEG images, it is worth reencoding those
with a lower quality setting to get a smaller PDF file. PDF enhancer can
reencode JPEG images in an existing PDF, but sometimes not all the images
hae to be reencoded. With GIMP it is possible to get a real-time preview of
the quality degradation before saving, by moving the quality slider.

Please note that some cameras don't encode JPEG files effectively when
saving to the memory card, and it is possible to save a lot of space by
reencoding on the computer, even with high quality settings.

\paragraph{Optimize poorly exported images}

Not all image processing programs pay attention to size of the image file
they save or export. They might not use compression by default; or they
compress with suboptimal settings; or (for EPS files) they try to save the
file in some compatibility mode, encoding and compressing the data poorly;
or they add lots of unneeded metadata. These poorly exported images make
\TeX{} and the drivers run slow, and they waste disk space (both on the
local machine and in the revision control repository). A good rule of thumb
to detect a poorly exported image
is to use \cmd{sam2p} to convert the exported image to JPEG and PNG
(\texttt{sam2p -c ijg:85 exported.img test.jpg};
\texttt{sam2p exported.img test.png}), and if any of these files is a lot
smaller than the exported image, then the image was exported poorly.

Converting the exported image with \cmd{sam2p} (to any of EPS, PDF, JPEG and
PNG) is a fast and effective way to reduce the exported image size. Please
note that \cmd{sam2p}, with its default settings, doesn't create the
smallest possible file, but it runs very quickly, and it creates an image
file which is small enough to be embedded to the temporary PDF.

\paragraph{Embed vector fonts instead of bitmap fonts}

Most fonts used with \TeX{} nowadays are available in Type\,1 vector format.
(These fonts include the Computer Modern families, the Latin Modern families,
the URW versions of the base 14 and some other Adobe fonts,
the \TeX{} Gyre families, the Vera families, the Palatino family, the
corresponding math fonts, and some symbol and drawing fonts.) This is a
signifcant shift from the original \TeX{} (+ \cmd{dvips}) concept, which used
bitmap fonts generated by \MF{}. While drivers still support embedding
bitmap fonts to the PDF, this is not recommended, because bitmaps (at 600
DPI) are larger than their vector equivalent, they render more slowly and
they look uglier in some PDF viewers.

\begin{table}
\captiontop{Font \texttt{.map} files used by various drivers and their
symlink targets (default first) in \TeX{}\,Live 2008}\label{tab:mapfiles}
\par\small\noindent\hfil
\begin{tabular}{@{}ll@{}}
\toprule
\emph{Driver} & Font \texttt{.map} file\\
\midrule
\cmd{xdvi} & \pkg{ps2pk.map} \\
\cmd{dvips}& \pkg{psfonts.map} $\to$\\
           & \pkg{psfonts\_t1.map} $|$ (\pkg{psfonts\_pk.map}) \\
pdf\TeX{}  & \pkg{pdftex.map} $\to$\\
           & \pkg{pdftex\_dl14.map} $|$ (\pkg{pdftex\_ndl14.map}) \\
\cmd{dvipdfm(x)}& \pkg{dvipdfm.map} $\to$\\
                & \pkg{dvipdfm\_dl14.map} $|$ (\pkg{dvipdfm\_ndl14.map}) \\
\bottomrule
\end{tabular}
\end{table}

If a font is missing from the font \texttt{.map} file, drivers tend to
generate a bitmap font automatically, and embed that. To make sure this
didn't happen, it is possible detect the presence of bitmap fonts in a PDF
by running \texttt{grep -a "/Subtype */Type3" doc.pdf}. Here is how to
instruct pdf\TeX{} to use bitmap fonts only (for debugging purposes):
\texttt{pdflatex "\string\pdfmapfile{}\string\input" doc}.
The most common
reason for the driver not finding a corresponding vector font is that the
\texttt{.map} file is wrong or the wrong map file is used. With
\TeX{}\,Live, the \cmd{updmap} tool can be used to regenerate the
\texttt{.map} files for the user, and the \cmd{updmap-sys} command
regenerates the system-level \texttt{.map} files. Table~\ref{tab:mapfiles}
shows which driver reads which \texttt{.map} file. Copying over
\pkg{pdftex\_dl14.map} to the current directory as the driver-specific
\texttt{.map} file
usually makes the driver find the font. Old \TeX{} distributions had quite
a lot of problems finding fonts, upgrading to \TeX{}Live\,2008 or newer is
strongly recommended.

Some other popular fonts (such as the Microsoft web fonts) are available in
TrueType, which is another vector format. \cmd{dvipdfm(x)} and pdf\TeX{} can
embed TrueType fonts, but \cmd{dvips} can't (it just dumps the
\texttt{.ttf} file to the \texttt{.ps} file, rendering it unparsable).

OpenType fonts with advanced features such as script and feature selection
and glyph substitution are supported by Unicode-aware \TeX{}-derivatives
such as \XeTeX{}. \cmd{dvipdfmx} also supports OpenType fonts.

\paragraph{Omit the base 14 fonts} The base 14 fonts are Times (in 4 styles,
Helvetica (in 4 styles), Courier (in 4 styles), Symbol and Zapf Dingbats.
To reduce the size of the PDF, it is possible to omit them from the
PDF file, because PDF viewers tend to have them. However, omitting the base 14
fonts is deprecated since PDF\,1.5. Adobe Reader 6.0 or newer,
and other PDF viewers (such as \cmd{xpdf} and \cmd{evince}) don't contain
those fonts either, but they can find them as system fonts. On Debian-based
Linux systems, those fonts are in the \pkg{gsfonts} package.

In \TeX\,Live, the directives \emph{pdftexDownloadBase14} and
\emph{dvipdfmDownloadBase14} etc.\ in the config file
\texttt{texmf-config/web2c/updmap.cfg} specify whether to embed the base 14
fonts. After modifying this config file (either the system-wide or the one in
\texttt{\$HOME/.texlive2008}) and running the \cmd{updmap} command, the
following font map files would be created:

\begin{description}

\item[pdftex\_dl14.map] Font map file for pdf\TeX{} with the base 14 fonts
  embedded. This is the default.
\item[pdftex\_ndl14.map] Font map file for pdf\TeX{} with the base 14 fonts
  omitted.
\item[pdftex.map] Font map file used by pdf\TeX{} by default.
Identical to one of the two above, based on the
\emph{pdftexDownloadBase14} setting.
\item[dvipdfm\_dl14.map] Font map file for \cmd{dvipdfm(x)} with the base 14
  fonts embedded. This is the default.
\item[dvipdfm\_ndl14.map] Font map file for \cmd{dvipdfm(x)} with the base 14
  fonts omitted.
\item[dvipdfm.map] Font map file used by \cmd{dvipdfm(x)} by default.
Identical to one of the two above, based on the
\emph{dvipdfmDownloadBase14} setting.

\end{description}

It is possible to specify the base 14 embedding settings without modifying
config files or generating \texttt{.map} files. Example command-line for
pdf\TeX{}: \texttt{pdflatex "\string\pdfmapfile{pdftex\_ndl14.map}\string\input"
doc.tex}.
However, this will display a warning \emph{No flags specified for
non-embedded font}. To get rid of this, use
\texttt{pdflatex "\string\pdfmapfile{=pdftex\_ndl14\_extraflag.map}\string\input"
doc.tex}
instead. Get the \texttt{.map} file from \cite{pdfsizeopt-extra}.

The \texttt{.map} file syntax for \cmd{dvipdfm} is different, but
\cmd{dvipdfmx} can use a \texttt{.map} file of pdf\TeX{} syntax. Example:
\texttt{dvipdfmx -f pdftex\_dl14.map doc.dvi}. Please note that
\cmd{dvipdfm} loads the \emph{.map} files specified in \pkg{dvipdfmx.cfg}
first, and the \texttt{.map} files loaded with the \texttt{-f} flag override
entries loaded previously, from the config file. To have the base 14 fonts
omitted, run \texttt{dvipdfmx -f pdftex\_ndl14.map -f
dvipdfmx\_ndl14\_extra.map dok.tex}. Get the last \texttt{.map} file from
\cite{pdfsizeopt-extra}. Without \pkg{dvipdfmx\_ndl14\_extra.map}, a bug in
\cmd{dvipdfm} prevents it from writing a PDF file without the font -- it
would embed a rendered bitmap font instead.

\paragraph{Subset fonts}

\emph{Font subsetting} is the process when the driver selects and embeds
only the glyphs of a font which are actually used in the document. Font
subsetting is turned on by default for \cmd{dvips}, \cmd{dvipdfm(x)} and
pdf\TeX{} when emitting glyphs produced by \TeX{}.


\subsection{Extra manual tweaks on \TeX{}-to-PDF compilation}

This sections shows a couple of methods to reduce the size of the PDF created
by a \TeX{} compilation manually. It is not necessary to implement these
methods if the temporary PDF gets optimized by \cmd{pdfsizeopy.py}
$+$ Multivalent, because this combination implements the methods discussed
here.

\paragraph{Set the ZIP compression level to maximum}

For pdf\TeX{}, the assignment \texttt{\string\pdfcompresslevel9} select
maximum PDF compression. With \TeX{}\,Live 2008, this is the default. Here
is how to specify it on the command-line:
\texttt{pdflatex "\string\pdfcompresslevel9 \string\input" doc}.
For \cmd{dvipdfm(x)}, the command-line flag \texttt{-z9} can be used to
maximize compression. This is also the default.

Please note that PDF supports redundancy elimination in many different
levels (see in Subsection~\ref{existing}). Setting the ZIP compression level is
only one of those.

There is no need to pay attention to this method, because
Multivalent recompresses all ZIP streams with maximum effort.

\paragraph{Generate object streams and cross-reference streams}

pdf\TeX{} can generate object streams and cross-reference streams to save
about 10\% of the PDF file size, or even more if the file contains lots of
hyperlinks. (The actual saving depends on the file
structure.) Example command-line for enabling it:
\texttt{pdflatex "\string\pdfminorversion5 \string\pdfobjcompresslevel3
\string\input" doc}.

There is no need to pay attention to this method, because
Multivalent generates object streams and cross-reference streams by default.

\paragraph{Encode Type\,1 fonts as CFF}

CFF \cite{cff} (\texttt{/Subtype /Type1C}; also known as Type\,2) 
is an alternative, compact, highly compressible binary font
format that can represent Type\,1 font data without loss. By embedding
vector fonts in CFFinstead of Type\,1, one can
save significant portion of the PDF file, especially if the document is 10
pages or less (e.g.\ reducing the PDFfile size from 200\,kB to 50\,kB).
\cmd{dvipdfmx} does this by default, the other drivers (pdf\TeX{},
\cmd{dvipdfm}, \cmd{ps2pdf} with \cmd{dvips}) don't support CFF embedding so
far.

There is no need to pay attention to this method, because
\cmd{pdfsizeopt.py} converts Type\,1 fonts in the PDF to CFF. 

\paragraph{Create graphics with font subsetting in mind}

For glyphs coming from external sources such as the included PostScript and
PDF graphics, the driver is usually not smart enough to recognize the fonts
already embedded, and unify them with the fonts in the main document. Let's
suppose that the document contains included graphics with text captions,
each graphics source PostScript or PDF having the font subsets embedded. No
matter \cmd{dvips}, \cmd{dvipdfm(x)} or pdf\TeX{} is the driver, it will not
be smart enough to unify these subsets to a single font. Thus space would be
wasted in the finally PDF file containing multiple subsets of the same font,
possibly storing duplicate versions of some glyphs.

It is posible to avoid this waste by using a graphics package implemented in
pure \TeX{} (such as TikZ) or using \MP{} (for which there is
special support in \cmd{dvips}, \cmd{dvipdfm(x)} and pdf\TeX{} to avoid font
and glyph duplication). The package \pkg{psfrag} doesn't suffer from this
problem either if the EPS files don't contain any fonts embedded.

There is no need to pay attention to this method, because
\cmd{pdfsizeopt.py} unifies font subsets.

\paragraph{Disable font subsetting before concatenation}

If a PDF document is a concatenation of several smaller PDF files (such as
in journal volumes and conference proceeding), and each PDF file contains
its own, subsetted fonts, then it depends on the concatenator tool whether
those subsets are unified or not. Most concatenator tools (\cmd{pdftk},
Multivalent, \pkg{pdfpages}, \pkg{ps2pdf}; see \cite{pdf-concatenate} for
more) don't unify these font subsets.

However, if you use \cmd{ps2pdf} for PDF concatenation, you can get font
subsetting and subset unification by \emph{disabling} font subsetting when
generating the small PDF files. In this case, Ghostscript (run by
\cmd{ps2pdf}) will notice that the document contains the exact same font
many times, and it will subset only one copy of the font.

There is no need to pay attention to this method, because
\cmd{pdfsizeopt.py} unifies font subsets.

\paragraph{Embed each graphics file once}

When \texttt{\string\includegraphics} is used multiple times on the same
graphics file (such as the company logo on presentation slides), it depends
on the driver whether the graphics data is duplicated in the final PDF.
pdf\TeX{} doesn't duplicate, \cmd{dvipdfm(x)} duplicates only \MP{}
graphics, and \cmd{dvips} always duplicates.

There is no need to pay attention to this method, because
both \cmd{pdfsizeopt.py} and Multivalent eliminate duplicates of identical
objects.



\subsection{How PDF optimizers save space}\label{existing}

!! what does Multivalent implement?

This section describes some methods PDF optimizers use the reduce the file
size. We focus on ideas and methods relevant to \TeX{} documents.

\paragraph{Use cross-reference streams and compress them with the $y$-predictor}

Each offset entry in an (uncompressed) cross-reference table consumes 20
bytes. This can be reduced by using compressed cross-reference streams, and
enabling the $y$-predictor. A reduction factor of 180 is possible if the
PDF file contains many objects (e.g.\ more than $10^5$ objects in
\emph{pdfref}, with less than 12000 bytes in the cross-reference stream).

The reason why the $y$-predictor can make a difference of a factor of 2 or
even more is the following.
The $y$-predictor encodes each byte in a rectangular array of bytes by
subtracting the original byte above the current byte from the current byte.
So if each row of the rectangular array contains an object offset, and the
offsets are increasing, then most of the bytes in the output of the
$y$-predictor would have a small absolute value, mostly zero. Thus the
output of the $y$-predictor can be compressed better with ZIP than
the original byte array.

Some tools such as Multivalent implement the $y$-predictor with PNG
predictor 12, but using TIFF predictor 2 avoids stuffing in the extra byte
per each row -- \cmd{pdfsizeopt.py} does that.

\paragraph{Use object streams}

It is possible to save space in the PDF by concatenating small (non-stream)
objects to an object stream, and compressing the stream as a whole. One can
even sort objects by type first, so similar objects will be placed next to
each other, and they will fit to the 32\,kB long ZIP compression window.

Please note that both object streams and cross-reference streams are
PDF\,1.5 features, and cross-reference streams must be also used when object
streams are used.

\paragraph{Use better stream compression}

In PDF any stream can be compressed with any compression filter (or a
combination of filters). ZIP is the most effective general-purpose
compression, which is recommended for compressing content streams, object
streams, cross-reference streams and font data streams (such as CFF).
For images, however, there are specialized filters (see later in this
section).

Most PDF generators (such as \cmd{dvipdfm(x)} and pdf\TeX{}) and
optimization tools (such as Multivalent)
use the \pkg{zlib} code for general-purpose ZIP compression.
\pkg{zlib} lets the user specify the \emph{effort} parameter between 0 (no
compression) and 9 (slowest compression, smallest output) to balance
compression speed versus compressed data size. There are, however
alternative ZIP compressor implementations (such as the one in
KZIP \cite{kzip} and PNGOUT \cite{pngout}), which
privide an even higher effort -- but the author doesn't know of any PDF
optimizers using those algorithms.

\paragraph{Recompress pixel-based images}

Since PDF supports more than 6 compression methods (and any combination of
them) and more than 6 predictors, there are lots of possibilities to make
images smaller. Here we focus on lossless compression (thus excluding JPEG
and JPEG2000 used for compressing photos). An image is rectangular array of
pixels. Each pixel is encoded as a vector of one or more components in the
color space of the image. Typical color spaces are grayscale
(\texttt{/DeviceGray}), RGB (\texttt{/DeviceRGB}), CMYK
(\texttt{/DeviceCMYK}), color spaces where colors are device-independent,
and the palette (indexed) versions of those. Each color component of each
pixel is encoded as a nonnegative integer with a fixed number of bits
(bits-per-component, BPC; can be 1, 2, 4, 8, 12 or 16). The image data can be
compressed with any combination of the PDF compression methods.

Before recompressing the image, ususally it is worth extracting the raw RGB or
CMYK (or device-independent) image data, and then compressing the image the
best we can. Partial approaches such as optimizing the palette only are
usually suboptimal, because they may be incapable of converting an indexed
image to grayscale to save the storage space needed by the palette.

To compress the image, we have to decide which color space,
bits-per-component, compression method(s) and predictor to use. We have to
choose a color space which can represent all the colors in the image. We may
convert a grayscale image to an RGB image (and back if all pixels are
grayscale). We may also convert a grayscale image to a CMYK image (and maybe
back). If the image doesn't have more than 256 different colors, we can use
an indexed version of the color space. A good rule of thumb (no matter the
compression) is to pick the color space $+$ bits-per-component combination
which needs the least number of bits per pixel. On a draw, pick the one
which doesn't need a palette. These ideas can also be applied if the image
contains an alpha channel (which allows for transparent or semi-transparent
pixels).

It is possible to further optimize some corner cases, for example if the
image has only a single color, then it is worth encoding it as vector
graphics filling a rectangle of that color. Or, when the image is a grid of
rectangles, where each rectangle contains a single color, then it is worth
encoding a lower resolution image, and increase the scale factor in the
image transformation matrix to draw the larger image.

A high-effort ZIP is the best compression method supported by PDF, except
for bilevel (two-color) images, where JBIG2 can yield a smaller result for
some inputs. JBIG2 is most effective on images with lots of 2D repetitions,
e.g.\ images containing lots of text (because the letters are repeating).
Other lossless compression methods supported by PDF (such as RLE, LZW and G3
fax) are inferior to ZIP and/or JBIG2. Sometimes the image is so small (like
$10\times10$ pixels) that compressing would increase its size.
Most of the images don't benefit from a predictor (used
together with ZIP compression), but some of them do. PDF supports the PNG
predictor image data format, which makes it possible to choose a different
predictor for scanline (image row). The heuristic default algorithm in
\cmd{pnmtopng} calculates all 5 scanline variations, and picks the one
having the smallest sum of absolute values. This facilitates bytes which
small absolute values in the uncompressed image data, so the Huffman coding
in ZIP can compress it effectively.

Most of the time it is not possible to tell in advance if ZIP or JBIG2
should be used, or whether a predictor should be used with ZIP or not. To
get the smallest possible output, it is recommended to run all 3 variations
and pick the one yielding the smallest image object. For very small images,
the uncompressed version should be considered as well. If the image is huge
and it has lots repetitive regions, it may be worth to apply ZIP more than
once. Please note that metadata (such as specifying the decompression
filter(s) to use) also contributes to the image size.

Most PDF optimizers use the \pkg{zlib} code for ZIP compression in images.
The output of some other image compressors (most notably PNGOUT
\cite{pngout}, see OptiPNG and others at \cite{png-recompressors})
is smaller than what \pkg{zlib} produces with its highest effort, but those
other compressors usually run a 100 times or even slower than \pkg{zlib}.

How much a document size decreases because of image recompression depends on
the structure of the document (how many images are there, how large the
images are, how large part of the file size is occupied by images) and how
effectively the PDF was generated. The percentage savings in the
\emph{image} column of Table~\ref{tab:psom-by-type} suggests that only a little
saving is possible (about 5\%) if the user pays attention to embed the
images effectively, according to the image-related guidelines presented in
Section~\ref{tex-to-pdf}. It is possible to save lots of space by decreasing
the image resolution, or decreasing the image quality by using some lossy
compression method (such as JPEG or JPEG2000) with lower quality settings.
These kind of optimizations are supported by Adobe Acrobat and PDF Enhancer,
but they are out of scope of our goals to decrease the file size while not
changing its rendered appearance.

\paragraph{Convert some inline images to objects}

It is possible to inline images into content streams. This PDF feature saves
about 30 bytes per image as compared to having the image as a standalone
image object. However, inline images cannot be shared. So in order to save
the most space, inline images which are used more than once should be
converted to objects, and image objects used only once should be converted
to inline images. Images having palette duplication with other images should
be image objects, so the palette can be shared.

\paragraph{Unify duplicate objects}

If two or more PDF objects share the same serialized value, it is natural to
save space by keeping only the first one, and modifying references to the
rest so that they refere to the first one. It is possible to optimize even
more by constructing equivalence classes, and keeping only one object per
class. For example, if the PDF contains
{\small
\begin{verbatim}
5 0 obj << /Next 6 0 R /Prev 5 0 R >> endobj
6 0 obj << /Next 5 0 R /Prev 6 0 R >> endobj
7 0 obj << /First 6 0 R >> endobj
\end{verbatim}}
\noindent then objects 5 and 6 are equivalent, so we can rewrite the PDF to
{\small
\begin{verbatim}
5 0 obj << /Next 5 0 R /Prev 5 0 R >> endobj
7 0 obj << /First 5 0 R >> endobj
\end{verbatim}}
%
PDF generators usually don't emit duplicate objects on purpose, but it just
happens by chance that some object values are equal. If the document
contains the same page content, font, font encoding, image or graphics more
than once, and the PDF generator fails to notice that, then these would most
probably become duplicate objects, which can be optimized away. The method
\cmd{dvips} $+$ \cmd{ps2pdf} usually produces lots of duplicated objects if
the document contains lots of duplicate content such as
\texttt{\string\includegraphics} loading same graphics many times.

\paragraph{Remove image duplicates, based on pixel value}

Different color space, bits-per-pixel and compression settings can cause
many different representations of the same image (rectangular pixel array)
be present in the document. This can indeed happen if different parts of the
PDF were created with different different (e.g.\ one with pdf\TeX{},
another with \cmd{dvips}), and the results were concatenated. To save space,
the optimizer can keep only the smallest image object, and update
references.

\paragraph{Remove unused objects}

Some PDF files contain objects which are not reachable from the
\texttt{/Root} object. This cruft may be present because of updates,
concatenations or conversion, or because the file is a linearized PDF. It is
save to save space by removing those unused objects. A
linearized PDF provides better web experience to the user, because it makes
the first page of the PDF appear earlier. Since a linearized PDF can be
automatically generated from a non-linearized one any time, it is no point
keeping a linearized PDF when optimizing for size.

\paragraph{Extract large parts of objects}

Unifying duplicate objects can save space only if a whole object is
duplicated. If a paragraph is repeated on a page, it will most probably
remain duplicated, because the duplication is within a single object (the
content stream). So the optimizer can save space by detecting content
duplication in the sub-object level (outside stream data and inside content
stream data), and extracting the duplicated parts to individual objects,
which can now be unified. Although this extraction would usually be too
slow if applied to all data structures in the PDF, it may be worth applying
it to some large structures such as image palettes (whose maximum size is
768 bytes for RGB images).

\paragraph{Reorganize content streams and form XObjects}

Instructions for drawing a single page can span over multiple content
streams and form XObjects. To save space, it is possible to concatenate
those to a single content stream, and compress the stream at once. After all
those concatenations, large common instruction sequences can be extracted
to form XObjects to make code reuse possible.

\paragraph{Remove unnecessary indirect references}

The PDF specification defines if a value withing a compound PDF value must
be an indirect reference. If a particular value in the PDF file is an
indirect reference, but it doesn't have to be, and other objects are not
referring to that object, then inlining the value of the object saves space.
Some PDF generators emit lots of unnecessary indirect references, because
they generate the PDF file sequentially, and for some objects they don't
know the full value when they are generating the object -- so they replace
parts of the value by indirect references, whose definitions they give
later. This strategy can save some RAM during the PDF generation, but it
makes the PDF about 40 bytes larger than necessary for each such reference.

\paragraph{Convert Type\,1 fonts to CFF}

Since drivers embed Type\,1 fonts to the PDF as Type\,1 (except for
\cmd{dvipdfmx}, which emits CFF), and CFF can represent the same font with
less bytes (because of the binary format and the smart defaults), and it also
and more compressible (because it doesn't have encryption), it is natural to
save space by converting Type\,1 fonts in the PDF to CFF.

\paragraph{Subset fonts}

This can be done by finding unused glyphs in fonts, and getting rid of them.
Usually this doesn't save any space for \TeX{} documents, because drivers
subset fonts by default.

\paragraph{Unify subsets of the same font}

As discussed in Section~\ref{tex-to-pdf}, a PDF file may end up containing
multiple subsets of the same font when typesetting a collection of articles
(such as a journal volume or a conference proceedings) with \LaTeX{}, or
embedding graphics containing text captions. Since these subsets are not
identical, unifying duplicate objects will not collapse them to a single
font. A font-specific optimization can save file size by taking a
union of these subsets in each font, thus eliminating glyph duplication and
improving compression effectiveness by grouping similar data (font glyphs)
next to each other.

\paragraph{Remove data ignored by the PDF specification}

For compatibility with future PDF specification versions, a PDF viewer or 
printer must accept dictionary keys which are not defined in the PDF
specification. These keys can be safely removed without affecting the
meaning of the PDF. An example for such a key is \texttt{/PTEX.Fullbanner}
emitted by pdf\TeX{}.

\paragraph{Omit explicitly specified default values}

The PDF specification provides default values for many dictionary keys. Some
PDF generators, however, emit key with the default value. It is safe to
remove these to save space.

\paragraph{Recompress streams with ZIP}

Uncompressing a stream and recompressing it with maximum-effort ZIP makes
the stream smaller most of the time. That's because ZIP is more effective
than the other general purpose compression algorithms PDF supports (RLE and
LZW).

For compatibility with the PostScript language, PDF supports the
\texttt{/ASCIIHexDecode} and \texttt{/ASCII85Decode} filters on streams.
Using these filters just makes the stream in the file longer (by a factor of
about 2/1 and 5/4, respectively). These filters make it possible to embed
binary stream data in a pure ASCII PDF file. However, there is no
significant use case for an ASCII-only PDF nowadays, so it is recommended to
get rid of these filters to decrease to file size.

\paragraph{Remove page thumbnails}

If the PDF file has page thumbnails, the PDf viewer can show them to the
user to make navigation easier and faster. Since page thumbnails are redundant
information which can be regenerated any time, it is safe to save space by
removing them.

\paragraph{Serialize values more effectively}

Whitespace can be omitted between tokens, except between a name token and a
token starting with a number or a letter (e.g.\ \texttt{/Ascent 750}).
Whitespace in front of \texttt{endstream} can be omitted as well. The binary
representation of strings should be used instead of the hexadecimal, because
it's never longer and it's shorter most of the time if used properly. Only
the characters \texttt{( \textbackslash\space)} have to be escaped with a
backslash within strings, but parentheses which nest can be left unescaped.
So, for example the string \texttt{a(()))(()\textbackslash b} can be represented
as
\texttt{(a(())\textbackslash)(\textbackslash(\textbackslash\textbackslash b)}.


\section{PDF size optimization tools}

\begin{table*}
\captiontop{PDF size reduction by object type, when running
\cmd{pdfsizeopy.py} + Multivalent}\label{tab:psom-by-type}
\advance\tabcolsep-2pt  % Prevent overfull \hbox.
\par\small\noindent\hfil
% !! which file benefits from which optimization !! why
\begin{tabular}{@{}lrrrrrr@{}}
\toprule
\emph{document} & \emph{contents} & \emph{font} & \emph{image} & \emph{other} & \emph{xref} & \emph{total} \\\midrule
cff & $141153-01$\% & $25547-01$\% & 0 & $178926-90$\% & $174774-99$\% & $521909-64$\% \\
beamer1 & $162423-01$\% & $44799-53$\% & $115160-00$\% & $453098-94$\% & $56752-97$\% & $832319-61$\% \\
eu2006 & $964248-00$\% & $3271206-90$\% & $3597779-05$\% & $531968-64$\% & $45792-93$\% & $8411464-42$\% \\
inkscape & $10676317-19$\% & $230241-00$\% & $6255203-19$\% & $946108-78$\% & $122274-93$\% & $18245172-23$\% \\
lme2006 & $1501584-13$\% & $314265-72$\% & $678549-05$\% & $176666-90$\% & $31892-92$\% & $2703119-24$\% \\
pdfref & $6269878-04$\% & $274231-03$\% & $1339264-00$\% & $17906915-78$\% & $6665536-99$\% & $32472771-64$\% \\
pgf2 & $2132674-00$\% & $275768-50$\% & 0 & $1183749-83$\% & $190832-95$\% & $3783193-35$\% \\
texbook & $1507901-00$\% & $519550-47$\% & 0 & $217616-83$\% & $35532-86$\% & $2280769-20$\% \\
tuzv & $112145-02$\% & $201155-83$\% & 0 & $21913-76$\% & $2471-87$\% & $337764-56$\% \\
\bottomrule
\end{tabular}
\par\bigskip
\par\noindent All numeric values are in bytes. !!
\end{table*}

\subsection{Test PDF files}

In order to compare the optimization efficiency of the tools presented in
this section, we have compiled a set of test PDF files, and optimized them
with each tool. The \emph{totals} column of Table~\ref{psom-by-type} shows
the size of each file (the $+$ and $-$ percentages can be ignored now), and
other columns show the bytes used by different object types. The test files
can be downloaded from \cite{example-pdfs}. Some more
details about the test files:

\begin{description}

\item[cff] 62-page technical documentation about the CFF file format. Font
data is a mixture of Type\,1, CFF and TrueType. Compiled with FrameMaker
7.0, PDF generated by Acrobat Distiller 6.0.1. 

\item[beamer1] 75 slide-steps long presentation created with
\pkg{beamer.cls} \cite{beamer}; containing hyperlinks, math formulas, some
vector graphics and a few pixel-based images. Compiled with pdf\TeX{}.
Font data is in Type\,1 format.

\item[eu2006] 126-page conference proceedings (of Euro\TeX{} 2006)
containing some large images. Individual articles were compiled with
pdf\TeX{}, and then PDF files were concatenated. Because of the
concatenation, many font subsets were embedded multiple times, so a large
part of the file is font data. Font data is mostly CFF, but it contains some
Type\,1 and TrueType fonts as well. Most fonts are compressed with the
less efficient LZW instead of ZIP.

\item[inkscape] 341-page software manual with lots of screenshots and small
images created with codeMantra \cite{codemantra}. Font data is a mixture of
Type\,1, CFF and TrueType.

\item[lme2006] 240-page conference proceedings in Hungarian, containing some
black-and-white screenshot images. Individual articles were compiled with
\LaTeX{} and \cmd{dvips} (without font subsetting), and the PostScript files
were concatenated and converted to PDF in a single run of a modified
\cmd{ps2pdf}. Since font subsetting was disabled in \cmd{dvips}, later
\cmd{ps2pdf} was able to subset fonts without duplication. Font data is in
CFF.

\item[pdfref] 1310-page reference manual about PDF\,1.7 containing quite a
lot of duplicate xref tables and XML metadata about document parts.
Optimization gets rid of both the duplicate xref tables and the XML
metadata. Font data is in CFF. Compiled with FrameMaker 7.2, PDF generated
by Acrobat Distiller 7.0.5.

\item[pgf2] 560-page software manual about TikZ, with lots of vector
graphics as examples, with an outline, without hyperlinks. Compiled with
pdf\TeX{}. Font data is in Type\,1 format.
!! did pdfe help in getting vector graphics smaller?

\item[texbook] 494-page user manual about \TeX{} (The \TeX{}book), compiled
with pdf\TeX{}. No pixel images, and hardly any vector graphics.

\item[tuzv] Mini novel in Hungarian, typeset on 20 A4 pages in a 2-column
layout, compiled with \cmd{dvipdfm}. It contains no images or graphics. Font data
is in Type\,1 format.

\end{description}

\noindent None of the test PDF files used cross-reference streams or object streams.


\begin{table}
\captiontop{PDF optimization efficiency
of \cmd{ps2pdf}}\label{tab:eff-ps2pdf}
\par\small\noindent\hfil
\begin{tabular}{@{}lrrrr@{}}
\toprule
\emph{document} & \emph{input} & \emph{ps2pdf} & \emph{psom} \\\midrule
cff         &   521909 &   264861 &   180987 \\
beamer1     &   832319 &  \emph{3027368} &   317351 \\
eu2006      &  8411464 &  6322867 &  4812306 \\
inkscape    & 18245172 &  failed  & 13944481 \\
lme2006     &  2703119 &  \emph{3091842} &  2033582 \\
pdfref      & 32472771 & 15949169 & 11237663 \\
pgf2        &  3783193 &  \emph{4023581} &  2438261 \\
texbook     &  2280769 &  \emph{2539424} &  1806887 \\
tuzv        &   337764 &   199279 &   146414 \\
\bottomrule
\end{tabular}
\par\bigskip
\par\noindent All numeric values are in bytes.
\par\noindent\emph{ps2pdf} Ghostscript 8.61 \texttt{ps2pdf14 -dPDFSETTINGS=/prepress}
\par\noindent\emph{psom:} \cmd{pdfsizeopt.py} $+$ Multivalent
\end{table}

\subsection{ps2pdf}

The \cmd{ps2pdf} \cite{ps2pdf} script (and its counterparts for specific PDF
versions, e.g.\ \cmd{ps2pdf14}) run Ghostscript with the flag
\texttt{-sDEVICE=pdfwrite}, which converts its input to PDF. Contrary to
what the name suggests, \cmd{ps2pdf} accepts not only PostScript, but PDF
files as input.

\cmd{ps2pdf} works by converting its input to low-level PostScript drawing
primitives, and then emitting them as a PDF document. \cmd{ps2pdf} doesn't
intend to be a PDF size optimizer, but it can be used as such.
Table~\ref{tab:eff-ps2pdf} shows that \cmd{ps2pdf} increases the file size
many times. For the documents \emph{cff} and \emph{pdfref}, we got a file
size decrease because \cmd{ps2pdf} got rid of some metadata, and for
\emph{pdfref}, it optimized cross-reference table. For \emph{eu2006} it
saved space by recompressing fonts with ZIP. The document \emph{tuzv} became
smaller because \cmd{ps2pdf} converted Type\,1 fonts to CFF. The reason for
the extremely large growth in \emph{beamer1} is that \cmd{ps2pdf} blew up
images, and it also embedded multiple instances of the same image as
separate images. (It doesn't always do so: if the two instances of the image
are close to each other, then \cmd{ps2pdf} reuses the same object in the PDF
for representing the image.)

\cmd{ps2pdf} keeps all printable features of the original PDF, and
hyperlinks and the document outline as well. However, it recompresses JPEG
images (back to a different JPEG, sometimes larger than the original), thus
losing quality. The only way to disable this is specifying the flags
\texttt{-dEncodeColorImages=false -dEncodeGrayImages=false}, but this would
blow up the file size even more, because it will keep photos uncompressed.

\cmd{ps2pdf} doesn't remove duplicate content (although it
removes image duplicates if they are closeby), and it also doesn't minimize
the use of indirect references (e.g.\ it emits the \texttt{/Length} of
content streams as an indirect reference). The only aspect \cmd{ps2pdf}
seems to optimize effectively is convert Type\,1 fonts to CFF. Since this
conversion is also done by \cmd{pdfsizeopt.py}, it is
not recommended to use \cmd{ps2pdf} to optimize PDF files.


\begin{table}
\captiontop{PDF optimization efficiency
of PDF Enhancer}\label{tab:eff-pdfe}
\par\small\noindent\hfil
\advance\tabcolsep-2pt  % Prevent overfull \hbox.
\begin{tabular}{@{}lrrrr@{}}
\toprule
% !! why is Multivalent alone better on pdf_reference_1-7? what about extra
%    compression
\emph{document} & \emph{input} & \emph{pdfe} & \emph{epsom} & \emph{psom} \\\midrule
cff         &   521909 &   229953 &   174182 &   180987 \\
beamer1     &   832319 &   756971 &   296816 &   317351 \\
eu2006      &  8411464 &  failed  & n/a      &  4812306 \\
inkscape    & 18245172 & 14613044 & 12289136 & 13944481 \\
lme2006     &  2703119 &  2263227 &  1781574 &  2033582 \\
pdfref      & 32472771 & 23794114 & 11009960 & 11237663 \\
pgf2        &  3783193 &  3498756 &  2245797 &  2438261 \\
texbook     &  2280769 &  2273410 &  1803166 &  1806887 \\
tuzv        &   337764 &   338316 &   147453 &   146414 \\
\bottomrule
\end{tabular}
\par\bigskip
\par\noindent All numeric values are in bytes.
\par\noindent\emph{pdfe:} PDF Enhancer 3.2.5 (1122r) server edition
\par\noindent\emph{epsom:} PDF Enhancer $+$ \cmd{pdfsizeopt.py} $+$ Multivalent
\par\noindent\emph{psom:} \cmd{pdfsizeopt.py} $+$ Multivalent
\end{table}


\subsection{PDF Enhancer}

!! menu name: Adobe Acrobat Pro 9 / Advanced / PDF Optimizer
!! Table~\ref{tab:eff-pdfe}
!! only the advanced server edition supports JBIG2

\begin{table}
\captiontop{PDF optimization efficiency
of Adobe Acrobat Pro 9}\label{tab:eff-a9}
\par\small\noindent\hfil
\advance\tabcolsep-2pt  % Prevent overfull \hbox.
\begin{tabular}{@{}lrrrr@{}}
\toprule
\emph{document} & \emph{input} & \emph{a9p4} & \emph{a9p5} & \emph{psom} \\\midrule
cff         &   521909 &   548181 &   329315 &   180987 \\
beamer1     &   832319 &   431936 &   334841 &   317351 \\
eu2006      &  8411464 &  8115676 &  7991997 &  4812306 \\
inkscape    & 18245172 & 14283567 & 13962583 & 13944481 \\
lme2006     &  2703119 &  2410603 &  2279985 &  2033582 \\
pdfref      & 32472771 & 23217668 & 20208419 & 11237663 \\
pgf2        &  3783193 &   failed &   failed &  2438261 \\
texbook     &  2280769 &  2314025 &  2150899 &  1806887 \\
tuzv        &   337764 &   344215 &   328843 &   146414 \\
\bottomrule
\end{tabular}
\par\bigskip
\par\noindent All numeric values are in bytes.
\par\noindent\emph{a9p4:} Adobe Acrobat Pro 9 creating PDF\,1.4
\par\noindent\emph{a9p5:} Adobe Acrobat Pro 9 creating PDF\,1.5
\par\noindent\emph{psom:} \cmd{pdfsizeopt.py} $+$ Multivalent
\end{table}



!! generated by pdfsizeopy.py --stats
!! inkscape: non-free book, 1st edition
!! inkscape: lots of small images
!! number of pages, number of images

\subsection{Adobe Acrobat}

See Table~\ref{tab:eff-a9}.


\begin{table}
\captiontop{PDF optimization efficiency
of Multivalent\,20060102}\label{tab:eff-multivalent}
\par\small\noindent\hfil
\advance\tabcolsep-2pt  % Prevent overfull \hbox.
\begin{tabular}{@{}lrrrr@{}}
\toprule
% !! why is Multivalent alone better on pdf_reference_1-7? what about extra
%    compression
\emph{document} & \emph{input} & \emph{multi} & \emph{psom} & \emph{pso} \\\midrule
cff         &   521909 &    181178 &    180987 &   230675 \\
beamer1     &   832319 &    341732 &    317351 &   443253 \\
eu2006      &  8411464 &   7198149 &   4812306 &  4993913 \\
inkscape    & 18245172 &  13976597 &  13944481 & 17183194 \\
lme2006     &  2703119 &   2285956 &   2033582 &  2349035 \\
pdfref      & 32472771 &  11235006 &  \emph{11237663} & 23413875 \\
pgf2        &  3783193 &   2584180 &   2438261 &  3449386 \\
texbook     &  2280769 &   2057755 &   1806887 &  1992958 \\
tuzv        &   337764 &    314508 &    146414 &   166863 \\
\bottomrule
\end{tabular}
\par\bigskip
\par\noindent All numeric values are in bytes.
\par\noindent\emph{multi:} Multivalent\,20060102 \emph{tool.pdf.Compress}
\par\noindent\emph{psom:} \cmd{pdfsizeopt.py} $+$ Multivalent
\par\noindent\emph{pso:} \cmd{pdfsizeopt.py} without Multivalent
\end{table}


\subsection{Multivalent tool.pdf.Compress}

!! licenses
!! does recompress ZIPs
!! Table~\ref{tab:eff-multivalent}
%!! Multivalent alone is better on pdf_reference_1-7
!! Example command line (creates optimized PDF as \texttt{doc-o.pdf}):
\texttt{java -cp Multivalent20060102.jar tool.pdf.Compress doc.pdf}

\subsection{pdfsizeopt.py}

\cmd{pdfsizeopt.py} \cite{pdfsizeopt} was written as part of this work. Its purpose is to
implement the most common optimizations typical \TeX{} documents benefit
from, but only those which are not already done by Multivalent. As described
in Section~\ref{workflow}, to get the smallest output PDF, the optimizations
done by \cmd{pdfsizeopt.py} should be applied first, and the result should
be processed by Multivalent. The 20060102 version of Multivalent
optimizes images, and it replaces the image even if the optimized version is
larger than the original, so \cmd{pdfsizeopt.py} implements a final step
to put those original images back which are smaller. 

\cmd{pdfsizeopt.py} can be used as a stand-alone PDF optimizer (without
Multivalent), but the final PDF will be much smaller if Multivalent
is run as well.

\cmd{pdfsizeopt.py} is free software licensed under the GPL. It is written
in Python. It needs Python\,2.4 (or 2.5 or 2.6). It uses only the standard
Python modules, but it invokes several external programs to help with the
optimizations. These are: Ghostscript (8.61 or newer is recommended),
\cmd{sam2p} \cite{sam2p} (0.46 is needed), pngtopnm,
the Multivalent compress tool \texttt{tool.pdf.Compress}
\cite{multivalent-compress-tool} (needs Sun's JDK or OpenJDK), 
optionally \cmd{jbig2} \cite{jbig2enc}, optionally PNGOUT \cite{pngout}.
Installation instructions are on \cite{pdfsizeopt-install}. Most of the
dependencies are free software, except for the Multivalent tools, which
are not free software or open source, but they can be downloaded and used on
the command line free of charge; for other uses they have to be licensed
commercially. PNGOUT is not free software or open source either, but the
binaries available free of charge can be used without restriction.

\cmd{pdfsizeopt.py} implements the following optimizations:

\begin{description}

\item[Convert Type\,1 fonts to CFF]
The actual conversion is done by generating a PostScript document with all
the fonts, converting it to PDF with Ghostscript (just like \cmd{ps2pdf}),
and extracting the CFF fonts from the PDF. Another option would be to use
\cmd{dvipdfmx}, which can read Type\,1 fonts, and emit them as CFF fonts.
Please note that Ghostscript inlines subroutines (\texttt{/Subrs}) in the
Type\,1 font, so the CFF becomes larger -- but we are compressing the font
with ZIP anyway, which eliminates most of the repetitions.

\item[Unify subsets of the same CFF font]
!!
Limitation: only for CFF (and former Type\,1) fonts.

\item[Convert inline images to objects]
!!
!! even those with only 1 reference
!! could not optimize otherwise
Limitation: only from sam2p.

\item[Optimize individual images]
Picks the best of: \cmd{sam2p} without predictor, \cmd{sam2p} with
PNG predictor, PNGOUT, \cmd{jbig2}.
!! hard to convert a PDF image to a PNG.
!! Limitations: no CMYK, only device-specific color spaces (RGB, Gray and
   RGB-palette)
!! no transparency

\item[Remove image duplicates]
!! based on image data

\item[Remove object duplicates]
!!

\item[Remove unused objects]
!!

\end{description}

\cmd{pdfsizeopt.py}



Improvement possibilites:

!! 

!! add concatenation

!! content stream and cross-reference stream support

!!!!!!!!

\section{Suggested PDF optimization workflow}\label{workflow}
!! use pdflatex or dvipdfmx with the right settings, use pdfsizeopt.py +
   Multivalent.

\section{Related work}\label{related-work}

An alternative document file format is DjVu (\cite{djvu,djvu-tutorial}),
whose most important limitation compared to PDF is that it doesn't support
vector graphics. Due to the sophisticated image layer separation and
compression, the size of a 600\,DPI DjVu file is comparable to the
coressponding optimized PDF document. If the PDF contains maily images (such
as a sequence of scanned sheets), the DjVu file will become slightly smaller
than the PDF file. If the PDF contains text with embedded vector fonts and
vector graphics, the DjVu file can be about 3 times larger than the PDF.
Of course these ratios depend on the software used for encoding as well.
There are only a few DjVu encoders available: \cmd{pdf2djvu} and
\cmd{djvudigital} are free, and Document\,Express is a commercial
application.

Since the DjVu file format uses very different technologies than PDF, one
can archive both the PDF and the DjVu version of the same document, in case
a decent renderer won't be available for one of the formats decades later.

!! more related work

\section{Conclusion}

!!

\section{References}

!! \cite all in info.txt

!! \cite one of Multivalent's articles

!! \cite the PDF compression web article

!! \cite the PDF optimization article and all from info.txt

@iso-pdf{
PDF 1.7 is ISO 32000
meta-link: http://www.theinquirer.net/inquirer/news/1030411/pdf-approved-iso-32000
}

@djvu-tutorial{
http://www.djvuzone.org/support/tutorial/chapter-intro.html
}

!!
@dvipdfmx{
http://project.ktug.or.kr/dvipdfmx/
}

% !! # in the URL
@pdfsizeopt-extra{
%http://code.google.com/p/pdfsizeopt/source/browse/#svn/trunk/extra
}

% !! # in the URL
@pts-graphics-helper{
%http://code.google.com/p/pdfsizeopt/source/browse/#svn/trunk/extra
}

@pstricks-pdfoutput{
http://tug.org/PSTricks/main.cgi?file=pdf/pdfoutput
}

@pdf-enhancer{
%http://www.apagoinc.com/prod_home.php?prod_id=2
}

@example-pdfs{
http://code.google.com/p/pdfsizeopt/wiki/ExamplePDFsToOptimize
}

@pdfsizeopt{
http://code.google.com/p/pdfsizeopt
}

@jbig2enc{
http://github.com/agl/jbig2enc/tree/master
}

@pdfsizeopt-install{
http://code.google.com/p/pdfsizeopt/wiki/InstallationInstructions
}

@multivalent-compress-tool{
!! write more about comparison
comparison: http://multivalent.sourceforge.net/Tools/pdf/Compress.html

% download: http://sourceforge.net/projects/multivalent/files/
% download: http://sourceforge.net/projects/multivalent/files/multivalent/Release%2020060102/Multivalent20060102.jar/download
}

@pdf-concatenate{
http://ansuz.sooke.bc.ca/software/pdf-append.php
}

@png-recompressors{
http://en.wikipedia.org/wiki/OptiPNG
}

@ps2pdf{
http://pages.cs.wisc.edu/~ghost/doc/cvs/Ps2pdf.htm
}

\end{document}
